{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Домашнее задание №3 по курсу «Машинное обучение»<center/>\n",
    "<center>*Кукуев Михаил*<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** <br/>Покажем, что $VCdim(H)\\geq d$ для семейства $H$ линейных бинарных классификаторов вида $h(x)=sign(w^Tx), w \\in R^d$.\n",
    "\n",
    "Возьмем в качестве набора $С$ ортонормированный базис пространства $R^d$: $C=\\{x_1, x_2, ..., x_n\\}$, где $x_i=(0,...,0,1^{(i)},0,...,0), ∀i= \\overline{1,d}$ и пусть $y_i \\in \\{-1, +1\\}$ - метка класса, к которому относится элемент $x_i \\in C$.\n",
    "\n",
    "В качестве гипотез из $H_C$ возьмем классификаторы вида $h(x)=sign(\\sum\\limits_{i=1}^d y_i x_i^T x)$, у которых\n",
    "вектор $w=\\sum\\limits_{i=1}^d y_i x_i, x_i \\in C$.\n",
    "\n",
    "Т.к. $x_i^T x_j=0$ только в случае $x_i \\neq x_j$, то выходит $h(x_j)=sign(y_j)$, и значит семейство таких гипотез разукрашивает $C$.\n",
    "\n",
    "Если же $|C|=d+1$, то хотя бы один из элементов набора $C$ можно представить в виде линейной комбинации остальных, т.к. пространство d-мерное. Любая гипотеза $h \\in H$ сможет вернуть для этого элемента только одно конкретное значение, причем заранее предопределенное значениями остальных элементов из $C$. Это значит, что нам не удастся получить $2^{d+1}$ различных наборов значений, возвращаемых функциями из $H$ для набора $С$, следовательно $H$ не разукрашивает $C$ размера $d+1$ и $VCdim(H) \\leq d$. Итого, $VCdim(H)=d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.**\n",
    "<br/>Так же, как и в предыдущей задаче, возьмем в качестве $С$ базис пространства: $C=\\{x_1^C,x_2^C, ..., x_n^C\\}$, где $x_i^C=(0,...,0,1^{(i)},0,...,0), ∀i= \\overline{1,n}$.\n",
    "Пусть $y_C=(y_1^C,y_2^C,...,y_n^C), y_j^C=\\{0,1\\}, ∀j= \\overline{1,n}$ - значение произвольной функции из $H_C$.\n",
    "Покажем, что для получения таких значений на наборе $С$ найдется функция из $H$.\n",
    "\n",
    "Выберем множество $I_C=\\{i:y_i^C=1\\}$, и ему соответствующую функцию из $H$: $h_{I_C}(x)=(\\sum\\limits_{i \\in I_C} x_i) mod2$. Заметим, что $h_{I_C}(x_j^C) = 1$, только если $j \\in I_C$, что эквивалентно условию $y_j^C=1$. Следовательно, семейство $H_C$ таких функций разукрашивает $C$ и $VCdim(H)\\geq n$.\n",
    "\n",
    "Так как размерность элементов множества $X$ равна n, то $|H| \\leq 2^n$. И т.к.  $VCdim(H) \\leq log_2(|H|)$, то $VCdim(H)=n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.**\n",
    "1. В NFL рассматриваются все $D$, когда говорится, что найдется такое $D$, для которого $L_D(A(S)) \\geq \\frac{1}{8}$. А утверждения из определения PAC-learnability выполняются не для всех $D$, а только на тех, для которых выполняется предположение о реализуемости.\n",
    "\n",
    "2. Полностью согласуются, agnostic PAC-learnability утверждает только что true risk найденной алгоритмом гипотезы $h$ больше наилучшей $h'$ максимум на $ε$, а NFL утверждает, что даже для наилучшей гипотезы $h'$ найдется такое $D$, что $L_D(h') \\geq \\frac{1}{8}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
